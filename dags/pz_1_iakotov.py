from datetime import datetime
import psycopg2 as pg
from io import BytesIO
import csv
import boto3 as s3
from botocore.client import Config
import codecs
import psycopg2 as pg
from airflow import DAG
from airflow.hooks.base import BaseHook
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import PythonOperator
import requests
import pendulum
import psycopg2 as pg
import ast

DEFAULT_ARGS = {
    'owner': 'admin',
    'retries': 2,
    'retry_delay': 600,
    'start_date': datetime(2024, 11, 12),
}
RAW_TABLE = 'iakotov_pz1_raw_table'
AGG_TABLE = 'iakotov_pz1_agg_table'

API_URL = "https://b2b.itresume.ru/api/statistics"


def load_from_api(**context):
    payload = {
        'client': 'Skillfactory',
        'client_key': 'M2MGWS',
        'start': context['ds'],
        'end': pendulum.parse(context['ds']).add(days=1).to_date_string(),
    }
    response = requests.get(API_URL, params=payload)
    data = response.json()

    connection = BaseHook.get_connection('conn_pg')

    with pg.connect(
            dbname='etl',
            sslmode='disable',
            user=connection.login,
            password=connection.password,
            host=connection.host,
            port=connection.port,
            connect_timeout=600,
            keepalives_idle=600,
            tcp_user_timeout=600
    ) as conn:
        cursor = conn.cursor()

        for el in data:
            row = []
            passback_params = ast.literal_eval(el.get('passback_params') if el.get('passback_params') else '{}')
            row.append(el.get('lti_user_id'))
            row.append(True if el.get('is_correct') == 1 else False)
            row.append(el.get('attempt_type'))
            row.append(el.get('created_at'))
            row.append(passback_params.get('oauth_consumer_key'))
            row.append(passback_params.get('lis_result_sourcedid'))
            row.append(passback_params.get('lis_outcome_service_url'))

            cursor.execute(f"INSERT INTO {RAW_TABLE} VALUES (%s, %s, %s, %s, %s, %s, %s)", row)

        conn.commit()


def aggregate_data(**context):
    sql_query = f"""
        INSERT INTO {AGG_TABLE}
        SELECT lti_user_id,
               attempt_type,
               COUNT(1),
               COUNT(CASE WHEN is_correct THEN NULL ELSE 1 END) AS attempt_failed_count,
               '{context['ds']}'::timestamp
          FROM {RAW_TABLE}
         WHERE created_at >= '{context['ds']}'::timestamp 
               AND created_at < '{context['ds']}'::timestamp + INTERVAL '1 days'
          GROUP BY lti_user_id, attempt_type;
    """

    connection = BaseHook.get_connection('conn_pg')

    with pg.connect(
            dbname='etl',
            sslmode='disable',
            user=connection.login,
            password=connection.password,
            host=connection.host,
            port=connection.port,
            connect_timeout=600,
            keepalives_idle=600,
            tcp_user_timeout=600
    ) as conn:
        cursor = conn.cursor()
        cursor.execute(sql_query)
        conn.commit()


def upload_data(table_name, file_name, **context):
    sql_query = f"""
        SELECT * FROM {table_name}
        WHERE date >= '{context['ds']}'::timestamp 
              AND date < '{context['ds']}'::timestamp + INTERVAL '1 days';
    """

    connection = BaseHook.get_connection('conn_pg')

    with pg.connect(
            dbname='etl',
            sslmode='disable',
            user=connection.login,
            password=connection.password,
            host=connection.host,
            port=connection.port,
            connect_timeout=600,
            keepalives_idle=600,
            tcp_user_timeout=600
    ) as conn:
        cursor = conn.cursor()
        cursor.execute(sql_query)
        data = cursor.fetchall()

    file = BytesIO()

    writer_wrapper = codecs.getwriter('utf-8')

    writer = csv.writer(
        writer_wrapper(file),
        delimiter='\t',
        lineterminator='\n',
        quotechar='"',
        quoting=csv.QUOTE_MINIMAL
    )

    writer.writerows(data)
    file.seek(0)

    connection = BaseHook.get_connection('conn_s3')

    s3_client = s3.client(
        's3',
        endpoint_url=connection.host,
        aws_access_key_id=connection.login,
        aws_secret_access_key=connection.password,
        config=Config(signature_version="s3v4"),
    )

    s3_client.put_object(
        Body=file,
        Bucket='default-storage',
        Key=f"{file_name}.csv"
    )


def upload_raw(**context):
    upload_data(RAW_TABLE, f"RAW_{context['ds']}.csv")


def upload_agg(**context):
    upload_data(AGG_TABLE, f"AGG_{context['ds']}.csv")


with DAG(
        dag_id="iakotov_pz1",
        tags=['4', 'admin'],
        schedule='@daily',
        default_args=DEFAULT_ARGS,
        max_active_runs=1,
        max_active_tasks=1
) as dag:
    dag_start = EmptyOperator(task_id='dag_start')
    dag_end = EmptyOperator(task_id='dag_end')

    load_from_api = PythonOperator(
        task_id='load_from_api',
        python_callable=load_from_api,
    )
    aggregate_data = PythonOperator(
        task_id='aggregate_data',
        python_callable=aggregate_data,
    )
    upload_raw = PythonOperator(
        task_id='upload_raw',
        python_callable=upload_raw,
    )
    upload_agg = PythonOperator(
        task_id='upload_agg',
        python_callable=upload_agg,
    )

    dag_start >> load_from_api >> aggregate_data >> upload_raw >> dag_end
    dag_start >> load_from_api >> aggregate_data >> upload_agg >> dag_end
